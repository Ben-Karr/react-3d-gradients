## The Setup: ##

Assume that we collected some data as input-output-pairs (_blue circles_) and we want to find a function that takes an input value and predicts the corresponding output. In this particular example we generated a hundret points $(x_i,y_i)$ such that $$y_i = 3 \cdot x_i^2 + 2 \cdot x_i + \epsilon_i$$ where the values $x_i$ are evenly distributed between $-10$ and $10$ and $\epsilon_i$ is some randomly generated noise (to resemble "natuarlly" collected data).
<br/><br/>
Since we know that the data has been generated by a quadratic function it makes sense to limit ourself to such functions. Thus the goal shall be to find $a$ and $b$ such that the quadratic $f_{a,b}$ (_red curve_) with $$f_{a,b}(x) = a\cdot x^2 + b\cdot x $$ _best predicts_ the data points.

## Basic Controls ##

We can use the sliders to change <b>a</b> and <b>b</b> and try to visually fit $f_{a,b}$ or use the <b>loss</b> value as an indicator of how good our prediction is — the lower the value the better. The function that maps a point $(a,b)$ to its loss value is called the <b>loss function</b> (see <b>Choose critic</b> for more details).
<br/><br/>
We can chose from different <b>critics</b>, which changes how the loss function measures the difference between the targets and the predictions. This also changes the magnitude of the loss value and likewise the shape of the <b>surface plot</b>, where each of the points on the surface is a point in $3$-dimensional space consisting of some $a$, $b$ and the loss of $(a,b)$. In particular the point that is given by the current $a, b$ and their loss is highlighted as a _white circle_.

## The Gradient Arrow ##

The 3D-plot also shows a representation of the negative gradient of the loss function at $(a,b)$ (_gray arrow_). The negative gradient is a vector in the (here $2$-dimensional) parameter space which points into the direction of a local minimum. 
This means that if we — starting from some point $(a,b)$ — step into the direction of the negative gradient at $(a,b)$: the loss decreases . There is one constraint to that: <b>The size of the step has to be small enough</b>. (Also see <b>Gradient Stepper</b> for more details).

## The Gradient Stepper ##

The control pannel provides a value <b>Gradient Magnitude</b> which is the length of the gradient vector at the current point. This value might be very high, such that if we'd step the whole length of the gradient we'd overstep and walk off the surface rather than towards the minimum. We can scale the gradient vector by chosing an appropriate <b>Learning Rate</b> and the value <b>Stepsize</b> shows the result.
The appropriate learning rate can vary vastly by the critic we chose as well as on how far away or close the current point is to the minimum. If the loss diverged using the sliders moves the current point back on the surface.


## The Critic: ##

A critic measures the average distance from the targets $y_i$ to the predictions $\hat{y_i}=f_{a,b}(x_i)$.
The three critics that are availabel are:

* Mean Absolute Error: $$\text{mae}(y,\hat{y})=\frac{1}{m}\sum_{i=1}^{m}|y_i - \hat{y_i}|,$$
* Mean Square Error: $$\text{mse}(y,\hat{y})=\frac{1}{m} \sum_{i=1}^{m}(y_i - \hat{y_i})^2,$$
* Root Mean Square Error: $$\text{rmse}(y,\hat{y})=\sqrt{\frac{1}{m}\sum_{i=1}^{m}(y_i - \hat{y_i})^2,}$$

where $m$ is the number of observed instances (100 here).

## The Loss: ##

If $\text{critic}$ is one of the critics above, then the loss of some $a$ and $b$ is: $\text{critic}(y,f_{a,b}(x))$ and the <b>loss function</b> maps each point $(a,b)$ to its loss: $$\text{loss}(a,b) = \text{critic}(y, f_{a,b}(x)).$$
The graph of the function $\text{loss}$ generates the loss surface in the 3D plot.

<br/>

<b>Note:</b> Finding values $a,b$ such that $\text{loss}(a,b)$ is minimal is what we mean by finding a quadratic function $f_{a,b}$ that "best" predicts the target values $y_i$.


## Gradient Stepper ##

The Gradient Stepper performs the steps of gradient descent with a chosen learning rate.

## What was Gradient Descent again? ##

The goal of <a href="https://en.wikipedia.org/wiki/Gradient_descent#Description" target="_blank" rel="noopener noreferrer">Gradient Descent</a> is to find the local minimum of some (multi-variable, differentiable) function $F$. The process can be described as:

* Start with some non-specific guess, $\alpha_0$ say,</li>
* calculate the gradient of $F$ at $\alpha_0$: $\nabla F(\alpha_0)$,</li>
* calculate the new guess as: $\alpha_1=\alpha_0 - \gamma \cdot \nabla F(\alpha_0)$,</li>
* if $0 < \gamma$ is small enough then $F(\alpha_1) < F(\alpha_0)$,</li>
* iterating that process converges to the local minimum.</li>

<b>Note:</b> For our stepper $\gamma$ is given by <b>Learning Rate</b> and the initial value $\alpha_0$ is $(1,1)$ by default. So the first step would calculate the gradient of $\text{loss}$ at $(1,1)$ which is: $\nabla \text{loss}(1,1)$.

## And what was the Gradient again? ##

In the case of a multi-variable function as here, the <a href="https://en.wikipedia.org/wiki/Gradient" target="_blank" rel="noopener noreferrer">gradient</a> of a function at some point is the vector of <a href="https://en.wikipedia.org/wiki/Partial_derivative" target="_blank" rel="noopener noreferrer">partial derivates</a> at that point. Lets pick $\text{mse}$ as our critic then the gradient of $\text{loss}$ at $(1,1)$ would be:
$$
\nabla \text{loss} (1,1) = \begin{pmatrix} \frac{\partial \text{loss}}{\partial a}(1,1) \newline \frac{\partial \text{loss}}{\partial b}(1,1) \end{pmatrix} \approx \begin{pmatrix} -7910.767 \newline -15.146 \end{pmatrix},
$$
so a vector in the $2$-dimensional parameter space. 

## The importance of the learning rate ##

If we follow that example it gets quiet obvious why we need $\gamma$. Assume we don't scale the gradient (set $\gamma = 1$) our second guess would be 
$$\begin{pmatrix} 1 \newline 1 \end{pmatrix} - 1 \cdot \nabla \text{loss} (1,1) \approx \begin{pmatrix} 1 \newline 1 \end{pmatrix} - \begin{pmatrix} -7910.767 \newline -15.146 \end{pmatrix} = \begin{pmatrix} -7909.767 \newline -14.146 \end{pmatrix},$$
which is clearly no improvement. So it's crutial to understand that the gradient only provides the direction into we have to step but not how big or small our step should be. Using a learning rate of 1E-4$=0.0001$ the first step produces:
$$ 
\begin{pmatrix} 1 \newline 1 \end{pmatrix} - \gamma \cdot \nabla\text{loss}(1,1) = \begin{pmatrix} 1 \newline 1 \end{pmatrix} - 0.0001 \cdot \begin{pmatrix} -7910.767 \newline -15.146 \end{pmatrix}   = \begin{pmatrix} 1.7910767 \newline 1.0015146 \end{pmatrix} 
$$ 
which works out much better. On the other hand: using a different critic say $\text{rmse}$ results in way smaller loss values, thus in smaller gradients and hence a bigger learning rate should be used to get reasonable step sizes.