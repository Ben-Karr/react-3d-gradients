import Modal from '@mui/material/Modal';

import 'katex/dist/katex.min.css';
import { InlineMath, BlockMath } from 'react-katex';

export function Modals({showInfo, switchInfoShow}) {
    return (
        <>
            <Modal open={showInfo['point_info']} onClose={(e)=>switchInfoShow(e,'point_info')}>
                <div className='modal'>
                    <h4>Setup:</h4>
                    <div>
                        Assume you collected some data points (<i>blue circles</i>) and want to find a function (<i>red curve</i>) that predicts those values. The particular points here have been generated by the following formular: <BlockMath math="y_i = 3\cdot x_i^2 + 2\cdot x_i \pm \epsilon_i"/> where the values <InlineMath math="x_i"/> are evenly distributed from <InlineMath math="-10" /> to <InlineMath math="10" /> and <InlineMath math="\epsilon_i"/> is some randomly generated noise. Since we know that the values have been generated by a quadratic function it makes that we restrict ourselfs here on finding <InlineMath math="a"/> and <InlineMath math="b"/> such that: <BlockMath math="f_{a,b}(x) = a \cdot x^2 + b\cdot x"/> "best" predicts the data points.
                    </div>

                    <h4>The Sliders for a and b:</h4>
                    <div>
                        You can use the sliders to change the values <InlineMath math="a"/> and <InlineMath math="b"/> and trie to find <InlineMath math="f_{a,b}"/> that (visually) best predicts the given points. You can also use the <i>Loss</i>-value as an indicator as it quantifies how far away the predicted points are from the target ones.
                        <br/>
                        <br/>
                        The point <InlineMath math="(a,b)"/> can also be found in the 3D-plot
                    </div>
                </div>
            </Modal>
            <Modal open={showInfo['critic_info']} onClose={(e)=>switchInfoShow(e,'critic_info')}>
                <div className='modal'>
                    <h4>The critic:</h4>
                    <div>
                        The critic measures the average distance from the targets <InlineMath math="y_i"/> to the predictions <InlineMath math="\hat{y_i}=f_{a,b}(x_i)"/>.
                        The three critics that are availabel here are:
                        <ul>
                            <li> Mean Absolute Error: <BlockMath math="\text{mae}(y,\hat{y})=\frac{1}{m}\sum_{i=1}^{m}|y_i - \hat{y_i}|"/></li>
                            <li> Mean Square Error: <BlockMath math="\text{mse}(y,\hat{y})=\frac{1}{m} \sum_{i=1}^{m}(y_i - \hat{y_i})^2"/> </li>
                            <li> Root Mean Square Error: <BlockMath math="\text{rmse}(y,\hat{y})=\sqrt{\frac{1}{m}\sum_{i=1}^{m}(y_i - \hat{y_i})^2}"/></li>
                        </ul>
                    </div>
                    <h4>The Loss:</h4>
                    <div>
                        The loss of some <InlineMath math="a"/> and <InlineMath math="b"/> is: <InlineMath math="\text{critic}(y,f_{a,b}(x))"/> where <InlineMath math="\text{critic}"/> is one of the three critics above. The loss function maps each point <InlineMath math="(a,b)"/> to its loss: <BlockMath math="\text{loss}(a,b) = \text{critic}(y, f_{a,b}(x))."/>
                    </div>
                    <div>
                        <b>Note:</b> Finding the values <InlineMath math="a,b"/> such that <InlineMath math="\text{loss}(a,b)"/> is minimal is what we mean by finding a quadratic function <InlineMath math="f_{a,b}"/> that "best" predicts the target values <InlineMath math="y_i"/>.
                    </div>
                </div>
            </Modal>
            <Modal open={showInfo['stepper_info']} onClose={(e)=>switchInfoShow(e,'stepper_info')}>
                <div className='modal'>
                    <h4>Gradient Stepper</h4>
                    <div>
                        The Gradient Stepper performs one step of gradient descent with the chosen learning rate.
                    </div>
                    <h4>What was Gradient Descent again?</h4>
                    <div>
                        The goal of <a href="https://en.wikipedia.org/wiki/Gradient_descent#Description" target="_blank" rel="noopener noreferrer">Gradient Descent</a> is to find the local minimum of some (multi-variable, differentiable) function <InlineMath math="F"/>. The process can be described as:
                        <ul>
                            <li>Start with a random guess for the local minimum, <InlineMath math="\alpha_0"/> say,</li>
                            <li>calculate the gradient of <InlineMath math="F"/> at <InlineMath math="\alpha_0" />: <InlineMath math="\nabla F(\alpha_0)"/>,</li>
                            <li>calculate the new guess for a local minimum as: <InlineMath math="\alpha_1=\alpha_0 - \gamma \cdot \nabla F(\alpha_0)"/>,</li>
                            <li>if <InlineMath math="0 < \gamma"/> is small enough then <InlineMath math="F(\alpha_1) < F(\alpha_0)"/>,</li>
                            <li>iterating that process converges to the local minimum.</li>
                        </ul>
                        <b>Note:</b> For our stepper <InlineMath math="\gamma"/> is given by the "Learning Rate" and the initial value <InlineMath math="\alpha_0"/> is <InlineMath math="(1,1)"/> by default. So the first step would calculate the gradient of <InlineMath math="\text{loss}"/> at <InlineMath math="(1,1)"/> which is: <InlineMath math="\nabla \text{loss}(1,1)"/>.
                    </div>
                    <h4>And what was the Gradient again?</h4>
                    <div>
                        In the case of a multi-variable function as here, the <a href="https://en.wikipedia.org/wiki/Gradient" target="_blank" rel="noopener noreferrer">gradient</a> of a function at some point is the vector of <a href="https://en.wikipedia.org/wiki/Partial_derivative" target="_blank" rel="noopener noreferrer">partial derivates</a> at that point. Lets pick <InlineMath math="\text{mse}"/> as our critic then the gradient of <InlineMath math="\text{loss}"/> at <InlineMath math="(1,1)"/> would be:
                        <BlockMath math="
                            \nabla \text{loss} (1,1) = \begin{pmatrix} \frac{\partial \text{loss}}{\partial a}(1,1) \\ \frac{\partial \text{loss}}{\partial b}(1,1) \end{pmatrix} \approx \begin{pmatrix} -7910.767 \\ -15.146 \end{pmatrix},
                        "/>
                        so a vector in the <InlineMath math="2"/>-dimensional parameter space. The value <i>Gradients Magnitude</i> in the control panel shows the length of this vector which should make it <b>obvious why we need a learning rate</b>. The <i>Stepsize</i> is the magnitude multiplied with the learning rate and is the acctual distance from the old point <InlineMath math="(1,1)"/> to the new one <InlineMath math="(1,1) - \gamma \cdot \nabla \text{loss}(1,1)"/>, so the length of the step we are taking.
                        <br/><br/>
                        <b>Note:</b> In the 3D plot the gradient vector is represented by a <i>gray arrow</i>. It's length scales with the gradients magnitude but is at least 0.5 and at most 2.
                    </div>
                </div>
            </Modal>
        </>
    )
}