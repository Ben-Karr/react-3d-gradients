import Modal from '@mui/material/Modal';

import 'katex/dist/katex.min.css';
import { InlineMath, BlockMath } from 'react-katex';

export function Modals({showInfo, switchInfoShow}) {
    return (
        <>
            <Modal open={showInfo['point_info']} onClose={(e)=>switchInfoShow(e,'point_info')}>
                <div className='modal'>
                    <h4>The Setup:</h4>
                    <div>
                        Assume that we collected some data as input-output-pairs (<i>blue circles</i>), then the goal here is to find a function that takes an input value and predicts the corresponding output. In this particular example we generated a hundret points <InlineMath math="(x_i,y_i)"/> such that <BlockMath math="y_i = 3 \cdot x_i^2 + 2 \cdot x_i + \epsilon_i"/> where the values <InlineMath math="x_i"/> are evenly distributed between <InlineMath math="-10"/> and <InlineMath math="10"/> and <InlineMath math="\epsilon_i"/> is some randomly generated noise.
                        <br/><br/>
                        Since we know that the data has been generated by a quadratic function it makes sense to limit ourself to such functions. Thus the goal shall be to find <InlineMath math="a"/> and <InlineMath math="b"/> such that the quadratic <InlineMath math="f_{a,b}"/> (<i>red curve</i>) with <BlockMath math="f_{a,b}(x) = a\cdot x^2 + b\cdot x "/> <i>best predicts</i> the data points.
                    </div>
                    <h4>Basic Controls</h4>
                    <div>
                        In order to do so we can use the sliders to change <b>a</b> and <b>b</b> and try to visually fit <InlineMath math="f_{a,b}"/> or use the <b>loss</b> value as an indicator of how good our prediction is — the lower the value the better. Lets call the function that maps a point <InlineMath math="(a,b)"/> to its loss value <InlineMath math="\text{loss}"/> (see <b>Choose critic</b> for more details).
                        <br/><br/>
                        We can chose from different <b>critics</b>, which changes how the loss function measures the difference between the targets and the predictions. This directly changes the magnitude of the loss value and likewise the shape of the surface plot as it — for every <InlineMath math="a"/> and <InlineMath math="b"/> — marks the loss that those values produce as a point <InlineMath math="(a,b,loss(a,b))"/> in <InlineMath math="3"/>-dimensional space. In particular the point that is given by the current <InlineMath math="a, b"/> and <InlineMath math="loss(a,b)"/> is highlighted as a <i>white circle</i>.
                    </div>
                    <h4>The Gradient Arrow</h4>
                    <div>
                        The 3D-plot also shows a representation of the gradient of the loss function at <InlineMath math="(a,b)"/> (<i>gray arrow</i>). The gradient is a vector in the (here <InlineMath math="2"/>-dimensional) parameter space which points into the direction of a local maximum. Accordingly the negative gradient points into the direction of a local minimum, meaning, that if we — starting from <InlineMath math="(a,b)"/> — step into the direction of the negative gradient: the loss decreases. There is one restriction to that: The step size has to be small enough.
                    </div>
                    <h4>The Gradient Stepper</h4>
                    <div>
                        Clicking the <b>Step</b> button performs such a step. We can controll the size of that step by changing the <b>Learning Rate</b> where you can take the <b>Gradients Magnitude</b> (length of the gradient) and the <b>Stepsize</b> as an indicator on which to pick. This can vary vastly by the critic we chose as well as how far away or close the current point is to the minimum.
                    </div>
                </div>
            </Modal>
            <Modal open={showInfo['critic_info']} onClose={(e)=>switchInfoShow(e,'critic_info')}>
                <div className='modal'>
                    <h4>The Critic:</h4>
                    <div>
                        The critic measures the average distance from the targets <InlineMath math="y_i"/> to the predictions <InlineMath math="\hat{y_i}=f_{a,b}(x_i)"/>.
                        The three critics that are availabel here are:
                        <ul>
                            <li> Mean Absolute Error: <BlockMath math="\text{mae}(y,\hat{y})=\frac{1}{m}\sum_{i=1}^{m}|y_i - \hat{y_i}|"/></li>
                            <li> Mean Square Error: <BlockMath math="\text{mse}(y,\hat{y})=\frac{1}{m} \sum_{i=1}^{m}(y_i - \hat{y_i})^2"/> </li>
                            <li> Root Mean Square Error: <BlockMath math="\text{rmse}(y,\hat{y})=\sqrt{\frac{1}{m}\sum_{i=1}^{m}(y_i - \hat{y_i})^2}"/></li>
                        </ul>
                    </div>
                    <h4>The Loss:</h4>
                    <div>
                        The loss of some <InlineMath math="a"/> and <InlineMath math="b"/> is: <InlineMath math="\text{critic}(y,f_{a,b}(x))"/> where <InlineMath math="\text{critic}"/> is one of the three critics above. The loss function maps each point <InlineMath math="(a,b)"/> to its loss: <BlockMath math="\text{loss}(a,b) = \text{critic}(y, f_{a,b}(x))."/>
                    </div>
                    <div>
                        <b>Note:</b> Finding the values <InlineMath math="a,b"/> such that <InlineMath math="\text{loss}(a,b)"/> is minimal is what we mean by finding a quadratic function <InlineMath math="f_{a,b}"/> that "best" predicts the target values <InlineMath math="y_i"/>.
                    </div>
                </div>
            </Modal>
            <Modal open={showInfo['stepper_info']} onClose={(e)=>switchInfoShow(e,'stepper_info')}>
                <div className='modal'>
                    <h4>Gradient Stepper</h4>
                    <div>
                        The Gradient Stepper performs one step of gradient descent with the chosen learning rate.
                    </div>
                    <h4>What was Gradient Descent again?</h4>
                    <div>
                        The goal of <a href="https://en.wikipedia.org/wiki/Gradient_descent#Description" target="_blank" rel="noopener noreferrer">Gradient Descent</a> is to find the local minimum of some (multi-variable, differentiable) function <InlineMath math="F"/>. The process can be described as:
                        <ul>
                            <li>Start with a random guess for the local minimum, <InlineMath math="\alpha_0"/> say,</li>
                            <li>calculate the gradient of <InlineMath math="F"/> at <InlineMath math="\alpha_0" />: <InlineMath math="\nabla F(\alpha_0)"/>,</li>
                            <li>calculate the new guess for a local minimum as: <InlineMath math="\alpha_1=\alpha_0 - \gamma \cdot \nabla F(\alpha_0)"/>,</li>
                            <li>if <InlineMath math="0 < \gamma"/> is small enough then <InlineMath math="F(\alpha_1) < F(\alpha_0)"/>,</li>
                            <li>iterating that process converges to the local minimum.</li>
                        </ul>
                        <b>Note:</b> For our stepper <InlineMath math="\gamma"/> is given by the "Learning Rate" and the initial value <InlineMath math="\alpha_0"/> is <InlineMath math="(1,1)"/> by default. So the first step would calculate the gradient of <InlineMath math="\text{loss}"/> at <InlineMath math="(1,1)"/> which is: <InlineMath math="\nabla \text{loss}(1,1)"/>.
                    </div>
                    <h4>And what was the Gradient again?</h4>
                    <div>
                        In the case of a multi-variable function as here, the <a href="https://en.wikipedia.org/wiki/Gradient" target="_blank" rel="noopener noreferrer">gradient</a> of a function at some point is the vector of <a href="https://en.wikipedia.org/wiki/Partial_derivative" target="_blank" rel="noopener noreferrer">partial derivates</a> at that point. Lets pick <InlineMath math="\text{mse}"/> as our critic then the gradient of <InlineMath math="\text{loss}"/> at <InlineMath math="(1,1)"/> would be:
                        <BlockMath math="
                            \nabla \text{loss} (1,1) = \begin{pmatrix} \frac{\partial \text{loss}}{\partial a}(1,1) \\ \frac{\partial \text{loss}}{\partial b}(1,1) \end{pmatrix} \approx \begin{pmatrix} -7910.767 \\ -15.146 \end{pmatrix},
                        "/>
                        so a vector in the <InlineMath math="2"/>-dimensional parameter space. The value <i>Gradients Magnitude</i> in the control panel shows the length of this vector which should make it <b>obvious why we need a learning rate</b>. The <i>Stepsize</i> is the magnitude multiplied with the learning rate and is the acctual distance from the old point <InlineMath math="(1,1)"/> to the new one <InlineMath math="(1,1) - \gamma \cdot \nabla \text{loss}(1,1)"/>, so the length of the step we are taking.
                        <br/><br/>
                        <b>Note:</b> In the 3D plot the gradient vector is represented by a <i>gray arrow</i>. It's length scales with the gradients magnitude but is at least 0.5 and at most 2.
                    </div>
                </div>
            </Modal>
        </>
    )
}